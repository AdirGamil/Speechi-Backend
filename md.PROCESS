# Process Documentation

## System Overview

The backend is a FastAPI application that implements a two-stage AI pipeline: **audio → transcription (Whisper) → summarization (Claude) → structured JSON**. The API exposes:

- `GET /api/health` — Liveness check
- `GET /api/supported-formats` — List of supported audio formats
- `POST /api/process-meeting` — JSON response (transcript + analysis)
- `POST /api/process-meeting/export-docx` — Word document download
- `POST /api/process-meeting/export-pdf` — PDF document download

All process-meeting endpoints accept an audio upload and an optional **language** form field (he, en, fr, es, ar; default en). They write audio to a temp file, call transcription then summarization (with language), and return either `APIResponse` (transcript + analysis, including `translated_transcript`), a `.docx`, or a `.pdf`. All file I/O is in `file_utils`; routes orchestrate only and map errors to HTTP status codes.

---

## Audio Input Strategy

### Supported Formats

The system accepts all common professional audio formats:

| Format | Extensions | Use Case |
|--------|------------|----------|
| MP3 | `.mp3` | Most common, good compression |
| WAV | `.wav` | Uncompressed, high quality |
| M4A | `.m4a` | Apple devices, AAC encoded |
| AAC | `.aac` | Advanced audio codec |
| OGG | `.ogg` | Open source, Vorbis/Opus |
| FLAC | `.flac` | Lossless compression |
| WebM | `.webm` | Browser recordings |
| MP4 | `.mp4` | Video format, audio track extracted |
| MPEG | `.mpeg`, `.mpg` | Legacy MPEG audio/video format |

### Upload vs Recording

The frontend supports two input methods:

1. **File Upload**: Traditional drag-and-drop or click-to-select. Works with all formats above.
2. **Browser Recording**: Uses MediaRecorder API. Outputs WebM or WAV depending on browser support.

Both methods produce a `File` object that is sent identically to the backend. **No special backend logic is needed for recordings** — they are treated exactly like uploaded files.

### Why No Transcoding

At this stage, we pass audio files directly to Whisper without transcoding because:

1. **Whisper handles multiple formats natively** — It uses FFmpeg internally and supports MP3, WAV, M4A, AAC, OGG, FLAC, and WebM.
2. **Transcoding adds latency** — Converting formats server-side would increase processing time.
3. **Quality preservation** — Avoiding unnecessary re-encoding maintains audio quality.
4. **Simplicity** — Direct passthrough reduces complexity and potential failure points.

If format issues arise in production, transcoding can be added as a preprocessing step without changing the API contract.

---

## Initial planning and system decomposition

The system was decomposed into clear layers before any AI integration:

1. **API layer** (`app/api/routes.py`): HTTP boundaries only. No business logic, no AI, no file logic.
2. **Services** (`transcription_service`, `summarization_service`, `document_service`): Each owns a single responsibility. They consume and produce simple types (paths, strings, Pydantic models) and do not depend on FastAPI.
3. **Utils** (`file_utils`, `env_utils`, `document_labels`): Cross-cutting concerns (temp files, env vars, i18n labels). No AI or HTTP.
4. **Models** (`schemas.py`): Contracts first. `ActionItem`, `AnalysisResult`, and `APIResponse` were defined before wiring Whisper or Claude so that prompt design and parsing could target a stable schema.

This separation made it possible to test each AI stage in isolation and to swap or upgrade models without touching the rest of the pipeline.

---

## AI usage

- **OpenAI Whisper:** **transcription only.** Audio → plain-text transcript. No summarization, no translation, no language selection.
- **Claude Sonnet:** **reasoning, summarization, and translation.** Receives transcript + target output language (he, en, fr, es, ar). Produces structured JSON: summary, participants, decisions, action items, and **translated_transcript**, all in the chosen language. No speech-to-text.

### Whisper Configuration

Whisper handles transcription only. It does not:
- Perform translation
- Identify speakers (speaker identification is heuristic: "Speaker 1", "Speaker 2")
- Generate summaries

This keeps the transcription stage focused and predictable.

---

## Long Transcript Handling (Chunked Processing)

### The Problem

Long recordings (up to 2+ hours) produce transcripts that can exceed model context limits or cause Claude to generate malformed JSON (unterminated strings, truncated output). This manifests as `ValueError: Claude response is not valid JSON: Unterminated string...`.

### Solution: Map-Reduce Chunking

For transcripts exceeding ~20,000 characters, the system uses a **chunked map-reduce** approach:

```
Long transcript (>20k chars)
    │
    ├── Split into chunks (~12k chars each, with 400-char overlap)
    │
    ├── MAP: Analyze each chunk independently
    │   ├── Chunk 1 → ChunkAnalysis (summary, participants, decisions, action_items, topics)
    │   ├── Chunk 2 → ChunkAnalysis
    │   └── Chunk N → ChunkAnalysis
    │
    └── REDUCE: Merge all ChunkAnalysis into final AnalysisResult
        ├── Synthesize overall summary
        ├── Deduplicate participants
        ├── Merge decisions (remove duplicates)
        ├── Merge action items (semantic deduplication)
        └── Generate condensed clean transcript
```

### Chunking Strategy

The `split_transcript()` function:
1. Splits on paragraph boundaries (`\n\n`) when possible
2. Falls back to single newlines, then sentence endings, then word boundaries
3. Never splits in the middle of a word
4. Maintains 400-character overlap between chunks for context preservation

### Output Schema Changes

The `AnalysisResult` schema now includes:
- `raw_transcript`: Always contains the full original Whisper output (never condensed)
- `is_condensed`: Boolean flag indicating if `translated_transcript` was condensed
- `language`: The output language code used

For long recordings, `translated_transcript` becomes a **condensed clean transcript** (structured narrative summary) rather than a full translation. Users can always access the full text via `raw_transcript`.

### JSON Repair Fallback

If initial JSON parsing fails, the system attempts automatic repair:

1. **Parse attempt 1**: Direct `json.loads`
2. **Parse attempt 2**: Extract JSON from markdown fences, fix trailing commas, escape newlines
3. **Parse attempt 3**: Call Claude with a "JSON repair" prompt to fix malformed output
4. **Final fallback**: Raise `ValueError` with diagnostic information

This multi-stage approach ensures reliable JSON output even when Claude produces formatting issues.

### Why Chunking Instead of Context Window Expansion

1. **Determinism**: Smaller chunks produce more consistent, predictable output
2. **Cost efficiency**: Processing in chunks is more cost-effective than extended context
3. **Error isolation**: If one chunk fails, others can still succeed
4. **Parallelization potential**: Chunks could be processed in parallel (future optimization)
5. **Quality**: Focused analysis on smaller segments often produces better extraction

---

---

## Document Export Architecture

### Single Source of Content

Document generation follows a **single content source, multiple formats** architecture:

```
analysis (AnalysisResult)
    ├── generate_word_document(analysis, language) → .docx
    └── generate_pdf_document(analysis, language) → .pdf
```

Both Word and PDF generators:
- Receive the same `AnalysisResult` object
- Use the same language-aware labels from `document_labels.py`
- Apply RTL formatting for Hebrew and Arabic
- Produce identical content structure

### Language-Aware Documents

All document section headings are localized:

```python
DOCUMENT_LABELS = {
    "en": {
        "title": "Meeting Summary",
        "transcript": "Transcript",
        "summary": "Summary",
        ...
    },
    "he": {
        "title": "סיכום פגישה",
        "transcript": "תמלול",
        ...
    },
    # fr, es, ar...
}
```

The backend receives the `language` parameter and passes it to document generation. Both Word and PDF respect:
- Localized headings
- RTL text direction for Hebrew/Arabic
- Proper alignment and paragraph direction

### PDF export: bug, font embedding, and WeasyPrint

**The PDF bug:** When PDFs were generated with ReportLab (canvas-based API), Hebrew and Arabic text rendered as **squares (■■■■)**. ReportLab’s default fonts do not include Hebrew/Arabic glyphs; without embedding fonts that support those scripts, the PDF engine falls back to a font that lacks the glyphs and shows replacement characters. This is a font embedding / script support issue, not a simple encoding bug.

**Why encoding “tricks” don’t fix it:** Ensuring UTF-8 in the pipeline fixes encoding on disk and in memory, but the PDF renderer still needs a font that contains the glyphs for the code points. Without such a font (and proper embedding), the result remains squares. A correct, production-ready fix therefore requires using a PDF stack that supports font embedding and multi-script fonts.

**Architectural decision — WeasyPrint:** We switched from ReportLab to **WeasyPrint** (HTML → PDF) because:

1. **HTML/CSS text stack** — WeasyPrint uses the same text layout as the web (Pango/Cairo), with proper Unicode and **font embedding**. We can load fonts (e.g. Heebo, Noto Sans Arabic, Noto Sans) via `@font-face` and they are embedded in the PDF, so Hebrew, Arabic, and Latin render correctly.
2. **RTL support** — HTML `dir="rtl"` and CSS `direction: rtl; text-align: right` give correct right-to-left layout for Hebrew and Arabic without ad-hoc hacks.
3. **No squares** — With the right fonts in `app/assets/fonts/` and `@font-face` in the template, all supported languages (Hebrew, Arabic, English, French, Spanish) render as readable text; no replacement characters.
4. **Production-ready** — Same content source as Word; language-aware headings and RTL are applied in one place (`pdf_service.py`). Temp file cleanup is done via `BackgroundTasks` after streaming the PDF.

**Font embedding:** Fonts (Heebo for Hebrew, Noto Sans Arabic for Arabic, Noto Sans for Latin) are loaded from local files under `app/assets/fonts/` and declared with `@font-face` in the HTML template. WeasyPrint embeds them in the PDF so the document is self-contained and displays correctly in Adobe Reader and browsers. See `app/assets/fonts/README.md` for which files to place where.

---

## Why OpenAI and Claude were split into separate stages

- **Different roles:** Whisper is built for speech-to-text. Claude is built for reasoning and structured output. Using each for its strength keeps the pipeline simple and predictable.
- **Easier debugging:** If the transcript is wrong, the bug is in transcription. If the summary is wrong or malformed, the bug is in summarization or the prompt. Mixed responsibilities would blur that line.
- **Independent testing:** Each AI service can be tested in isolation without running the full stack.
- **Operational flexibility:** Transcripts can be cached, re-summarized with different prompts, or fed into other tools without re-running Whisper. The split makes those options straightforward.

---

## Isolation testing before integration

During development, each AI stage was validated independently before wiring the full pipeline. This caught issues (missing keys, bad paths, prompt drift) early and kept integration minimal. The test scripts have been removed from the production codebase, but the architecture supports easy re-creation if needed.

---

## Prompt design and strict JSON enforcement

The meeting-analysis prompt (`app/prompts/meeting_summary_prompt.txt`) is designed as follows:

- **Inputs:** Raw transcript + target output language (ISO 639-1: he, en, fr, es, ar).
- **Responsibilities:** Summarize, extract participants, decisions, action items; produce a **translated, clean transcript** in the target language.
- **Rules:** Transcript-only; no hallucinations; empty arrays when info is missing; **valid JSON only**—no markdown, no code fences, no commentary; **do not mix languages** (all output in target language).
- **Output schema:** `summary`, `participants`, `decisions`, `action_items`, `translated_transcript`. Schema mirrors `AnalysisResult` and `ActionItem`.
- **Edge case (insufficient transcript):** `summary`: "Transcript too short or unclear to summarize."; empty arrays; `translated_transcript`: original transcript translated as-is.

We send "Target output language: {code}" and the transcript in the user message. We use `json.loads` on the (optionally fence-stripped) Claude response and `AnalysisResult.model_validate(data)`. Invalid JSON or schema mismatch → `ValueError` → 400.

---

## Why translation is done in the LLM, not Whisper or frontend

- **Whisper** is built for speech-to-text only. It does not take a "target language" for structured output; we keep it single-purpose.
- **Frontend** would require a separate translation step (e.g. another API or client-side translation) and would duplicate logic. Translation is inherently part of "understanding + restructuring" the meeting.
- **Claude** already receives the transcript for summarization and extraction. Adding translation in the same call keeps the pipeline simple, ensures summary and translated transcript are coherent, and avoids extra services or latency. One model, one pass, one schema.

---

## Markdown code-block issue and how it was addressed

**The problem:** LLMs sometimes wrap JSON in markdown code fences (` ```json ... ``` `) even when explicitly instructed not to. This breaks `json.loads` and causes 400 errors in the API, making the system brittle and unsuitable for production.

**Why prompt-only wasn't enough:** While the system prompt forbids markdown and provides examples of raw JSON, Claude occasionally still wraps responses in code fences. This is a known LLM behavior pattern: models trained on code-heavy datasets default to markdown formatting for structured output. Relying solely on prompt compliance creates a reliability gap.

**The solution:** A minimal defensive guard (`_extract_json` in `summarization_service.py`) strips markdown code fences if present, then passes the cleaned string to `json.loads`. The function:
- Trims whitespace.
- Removes leading ` ```json ` or ` ``` ` markers.
- Removes trailing ` ``` ` markers.
- Returns the cleaned string unchanged if no fences are present.

This improves robustness without weakening prompt discipline. The prompt still enforces raw JSON (reducing the need for stripping), but the guard ensures the API never fails due to markdown wrapping. Invalid JSON still raises `ValueError` as expected; we only handle the formatting edge case.

**Frontend readiness:** The backend is designed to plug into a React frontend later. By ensuring responses are always clean JSON (no markdown artifacts), we avoid frontend parsing complexity and keep the API contract stable.

---

## Final system flow validation

End-to-end flow:

1. **Upload/Record** → `POST /api/process-meeting` (or export-docx/export-pdf) with audio file and optional `language` (default en).
2. **Validation** → `file_utils.validate_audio_file(upload)` checks extension and MIME type.
3. **Temp file** → `file_utils.write_temp_audio`; suffix from filename.
4. **Transcribe** → `transcription_service.transcribe_audio(path)` → Whisper returns plain text.
5. **Summarize** → `summarization_service.analyze_transcript(transcript, language)` → Claude returns JSON (summary, participants, decisions, action_items, **translated_transcript**); we parse and validate as `AnalysisResult`.
6. **Response** → `APIResponse(transcript=..., analysis=...)` as JSON, or **Document** → `document_service.generate_word_document(analysis, language)` or `pdf_service.generate_pdf_document(analysis, language)`, then stream file.
7. **Cleanup** → `file_utils.delete_temp_file(path)` in `finally`; temp document via `BackgroundTasks` for export.

**E2E validation:** Run `curl -X POST -F "audio=@meeting.mp3" -F "language=he" .../api/process-meeting` and confirm `analysis.translated_transcript` and summary are in Hebrew. Run export-docx or export-pdf with `language=he` and confirm the document is in Hebrew with RTL formatting. Repeat for other supported languages.

---

## How This Exceeds Task Requirements

This implementation goes beyond the basic requirements:

1. **Professional audio format support** — 9 formats including MPEG.
2. **Long recording support** — Handles 2+ hour recordings via chunked processing.
3. **Browser recording** — Users can record directly without external software.
4. **Dual export formats** — Both Word and PDF with identical content.
5. **Full i18n** — 5 languages with complete RTL support.
6. **Language-aware documents** — Headings, labels, and text direction adapt to selected language.
7. **Clean architecture** — Single content source, multiple output formats.
8. **Input validation** — Extension and MIME type checking with clear error messages.
9. **Language-aware filenames** — `meeting_summary_he.docx` instead of generic names.
10. **Robust JSON parsing** — Multi-stage repair with Claude fallback for reliability.
11. **Dual transcript output** — Both condensed/translated and full original transcripts.

---

## Pipeline orchestration (reference)

The `POST /api/process-meeting` handler:

1. Validates `language` (he, en, fr, es, ar); 400 if unsupported.
2. Validates audio file format; 400 if unsupported.
3. Reads uploaded bytes, rejects empty file.
4. Uses `file_utils.suffix_from_filename` and `write_temp_audio` to create a temp file.
5. Calls `transcription_service.transcribe_audio(path)` then `summarization_service.analyze_transcript(transcript, language)`.
6. Returns `APIResponse(transcript=..., analysis=...)`.
7. Maps `ValueError` → 400, `RuntimeError` → 503.
8. Always runs `file_utils.delete_temp_file(path)` in `finally`.

Export endpoints: same pipeline, then `document_service.generate_word_document` or `pdf_service.generate_pdf_document`, `FileResponse` with language-aware filename, and `BackgroundTasks` for document cleanup.

---

## Challenges encountered

- **JSON enforcement:** Getting Claude to output *only* valid JSON, with no markdown or extra text, so the API can parse and validate reliably.
- **Markdown-wrapped responses:** Claude occasionally wraps JSON in ` ```json ... ``` ` despite instructions, breaking `json.loads`.
- **Long transcript failures:** 2-hour recordings cause Claude to produce truncated/malformed JSON due to context limits.
- **MPEG format support:** Some clients send `video/mpeg` MIME type for audio files, requiring flexible validation.
- **Isolation testing:** Validating Whisper and Claude independently before wiring the full pipeline, without running the full stack for each stage.
- **Language consistency:** Ensuring summary, extracted fields, and translated transcript are all in the selected output language, with no mixed-language leakage.
- **Separation of concerns:** Keeping transcription, summarization/translation, and document export in distinct services; no AI or file logic in routes.
- **RTL document formatting:** Proper right-to-left text direction and alignment for Hebrew and Arabic in both Word and PDF.
- **Browser recording compatibility:** Handling different MIME types across browsers (WebM vs WAV).

---

## Solutions applied

| Challenge | Solution |
|-----------|----------|
| Ensuring Claude outputs valid JSON only | Strict prompt rules with explicit JSON formatting instructions, no-markdown examples, and direct `json.loads` + Pydantic validation. |
| Claude wrapping JSON in markdown code fences | Minimal `_extract_json` helper strips ` ```json ` / ` ``` ` markers if present, ensuring `json.loads` always receives clean input. Prompt discipline remains; guard handles edge cases. |
| Long transcript failures (2+ hours) | Map-reduce chunking: split transcript into ~12k char chunks, analyze each independently, merge results. Includes automatic JSON repair via Claude fallback. |
| MPEG format support | Added `.mpeg` and `.mpg` extensions with `audio/mpeg`, `audio/x-mpeg`, and `video/mpeg` MIME types to `ALLOWED_AUDIO_FORMATS`. |
| Keeping routes free of business logic | All AI and file logic in services/utils; routes only orchestrate and map errors. |
| Validating each AI stage before integration | Each AI service tested independently during development before wiring the full pipeline. |
| Temp file cleanup on all paths | `try` / `finally` around transcribe + summarize; `delete_temp_file` always runs. |
| Frontend-ready API responses | Clean JSON extraction ensures responses are parseable without preprocessing, suitable for React or other frontend frameworks. |
| Language consistency | Prompt enforces "do not mix languages"; all output (summary, participants, decisions, action_items, translated_transcript) in target language. Routes validate `language` against allowed set. |
| Separation of concerns | AI only in services; routes validate input, orchestrate, map errors. Document service consumes `AnalysisResult` only, uses both `translated_transcript` and `raw_transcript` for documents. |
| RTL document formatting | `document_labels.py` provides `is_rtl()` helper; document generators apply RTL paragraph direction and right alignment. |
| Multiple audio formats | Centralized `ALLOWED_AUDIO_FORMATS` in `file_utils.py`; validation checks both extension and MIME type. Now includes 9 formats. |
| Browser recording | Frontend uses MediaRecorder API; output treated identically to uploads (same API endpoint). |
| JSON repair for malformed output | Three-stage repair: direct parse → fence stripping + character fixes → Claude repair prompt. Ensures reliable JSON even with LLM formatting issues. |

---

## Time Tracking (approximate)

| Stage | Focus | Time |
|-------|--------|------|
| Project setup | Structure, schemas, config, empty services | 1–2 h |
| FastAPI skeleton | App, CORS, router, health | ~30 min |
| Transcription service | Whisper integration, env, error handling | ~1 h |
| Prompt design | System prompt, schema alignment, examples | ~1 h |
| Summarization service | Claude, prompt loading, JSON parse, validation | ~1 h |
| Pipeline wiring | Route, file_utils, e2e flow | ~1 h |
| Isolation tests | `test_transcription.py`, `test_summarization.py` | ~30 min |
| Docs and final validation | README, md.PROCESS, sanity checks | ~1 h |
| Multi-language & translation | Prompt (language + translated_transcript), schema, summarization, routes, document_service, README, md.PROCESS | ~1–2 h |
| Extended audio formats | file_utils validation, route updates, frontend constants | ~1 h |
| PDF export | ReportLab integration, RTL support, document_service | ~2 h |
| Language-aware documents | document_labels, Word/PDF generators, headings | ~1–2 h |

Total on the order of **10–14 hours** for complete implementation.

---

## Document export

Document export is implemented as a **separate service** (`document_service`) that consumes `analysis: AnalysisResult` and writes either `.docx` or `.pdf` to a temp path. It uses **`analysis.translated_transcript`** for the Transcript section so the document output matches the selected output language. Pipeline: **audio → Whisper → transcript → Claude → analysis → (optional) Word/PDF**. Document generation is a pure, post‑AI step: no LLM calls, no file I/O outside `file_utils` (we use `delete_temp_file` for cleanup on error), and no FastAPI coupling. This keeps the pipeline modular.

---

## E2E validation

**Full pipeline (single flow):**  
`audio → Whisper → transcript → Claude → analysis → optional Word/PDF`

- **`POST /api/process-meeting`** — Form: `audio`, `language` (default en). Returns JSON: `transcript`, `analysis` (including `translated_transcript`). Use for frontend or downstream services.
- **`POST /api/process-meeting/export-docx`** — Form: `audio`, `language`. Same pipeline, then `document_service.generate_word_document(analysis, language)`, stream `meeting_summary_{lang}.docx`; cleanup temp audio in `finally`, temp docx via `BackgroundTasks`.
- **`POST /api/process-meeting/export-pdf`** — Form: `audio`, `language`. Same pipeline, then `pdf_service.generate_pdf_document(analysis, language)`, stream `meeting_summary_{lang}.pdf`; cleanup temp audio in `finally`, temp pdf via `BackgroundTasks`.

Document exports are **optional and separate** so that (1) the default API stays JSON-only and frontend-friendly, (2) document generation remains a post‑AI step with no AI in routes, and (3) all endpoints reuse the same pipeline. **Language selection** affects summary, `translated_transcript`, and document content/headings. E2E checks: `curl -X POST -F "audio=@meeting.mp3" -F "language=he" .../api/process-meeting` shows Hebrew in `analysis`; `.../export-docx` or `.../export-pdf` with `language=he` yields documents in Hebrew with RTL formatting.

---

## Environment Separation Strategy

### Why Environment Variables

All configuration is externalized to environment variables for:

1. **Security** — API keys and secrets never committed to version control
2. **Flexibility** — Same codebase runs in development, staging, and production
3. **12-Factor App compliance** — Configuration stored in the environment
4. **CI/CD readiness** — Easy to inject secrets in deployment pipelines
5. **Docker/Kubernetes** — Standard way to configure containers

### Frontend Environment

The frontend uses Vite's environment variable system:

| Variable | Purpose | Example |
|----------|---------|---------|
| `VITE_API_BASE_URL` | Backend API URL | `http://127.0.0.1:8000/api` |
| `VITE_APP_ENV` | Environment name | `development` or `production` |

**Files:**
- `.env.example` — Template (committed)
- `.env` — Local development (gitignored)
- `.env.production` — Production build (gitignored)

**Usage in code:**
```typescript
const API_BASE = import.meta.env.VITE_API_BASE_URL;
```

### Backend Environment

The backend uses pydantic-settings for type-safe environment loading:

| Variable | Purpose | Example |
|----------|---------|---------|
| `APP_ENV` | Environment name | `production` |
| `APP_HOST` | Server bind address | `0.0.0.0` |
| `APP_PORT` | Server port | `8000` |
| `CORS_ORIGINS` | Allowed origins (comma-separated) | `https://speechi.adirg.dev` |
| `OPENAI_API_KEY` | Whisper API key | `sk-...` |
| `ANTHROPIC_API_KEY` | Claude API key | `sk-ant-...` |

**Files:**
- `.env.example` — Template (committed)
- `.env` — Actual config (gitignored)

**Usage in code:**
```python
from app.config.settings import settings
print(settings.cors_origins_list)  # Parsed list of origins
```

---

## Why Frontend and Backend are Decoupled

### Architectural Decision

Frontend and backend are completely independent applications that communicate only via HTTP API:

```
┌─────────────┐         HTTP/HTTPS         ┌─────────────┐
│   Frontend  │ ◄──────────────────────────►│   Backend   │
│   (React)   │    JSON / FormData / Blob   │  (FastAPI)  │
└─────────────┘                             └─────────────┘
     ↓                                           ↓
 Static files                               Python runtime
 (CDN, Nginx)                              (uvicorn, Docker)
```

### Benefits

1. **Independent Deployment** — Frontend can be deployed to CDN without touching backend
2. **Independent Scaling** — Backend can scale horizontally without affecting frontend
3. **Technology Freedom** — Frontend and backend can use different tech stacks
4. **Team Independence** — Frontend and backend teams can work in parallel
5. **Caching** — Frontend static files cached at edge; API responses can be cached separately
6. **Security** — Frontend has no access to backend secrets; CORS enforces origin restrictions

### Production Architecture

```
                    ┌─────────────────┐
                    │   Load Balancer │
                    │   (SSL/HTTPS)   │
                    └────────┬────────┘
                             │
            ┌────────────────┴────────────────┐
            │                                 │
            ▼                                 ▼
    ┌───────────────┐               ┌───────────────┐
    │   Frontend    │               │   Backend     │
    │   (Nginx)     │               │   (FastAPI)   │
    │   /           │               │   /api        │
    └───────────────┘               └───────────────┘
            │                                 │
            ▼                                 ▼
    ┌───────────────┐               ┌───────────────┐
    │  Static Files │               │  OpenAI API   │
    │  dist/        │               │  Anthropic    │
    └───────────────┘               └───────────────┘
```

---

## How Environment-Based Configuration Enables Scaling

### Docker Support

Both frontend and backend can be containerized with environment variables:

**Backend Dockerfile:**
```dockerfile
FROM python:3.12-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
ENV APP_ENV=production
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Frontend Dockerfile:**
```dockerfile
FROM node:20-alpine AS build
WORKDIR /app
COPY package*.json ./
RUN npm ci
COPY . .
ARG VITE_API_BASE_URL
ENV VITE_API_BASE_URL=$VITE_API_BASE_URL
RUN npm run build

FROM nginx:alpine
COPY --from=build /app/dist /usr/share/nginx/html
```

### CI/CD Pipeline

Environment variables can be injected at build/deploy time:

```yaml
# GitHub Actions example
jobs:
  deploy:
    env:
      VITE_API_BASE_URL: https://speechi.adirg.dev/api
    steps:
      - run: npm run build
      - run: deploy-to-cdn dist/
```

### Kubernetes

Environment variables map naturally to ConfigMaps and Secrets:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: speechi-backend
data:
  APP_ENV: production
  CORS_ORIGINS: https://speechi.adirg.dev
---
apiVersion: v1
kind: Secret
metadata:
  name: speechi-secrets
stringData:
  OPENAI_API_KEY: sk-...
  ANTHROPIC_API_KEY: sk-ant-...
```

---

## Production Deployment Checklist

### Before Deployment

- [ ] All environment variables documented in `.env.example`
- [ ] No hardcoded URLs in frontend code
- [ ] No hardcoded URLs in backend code
- [ ] No secrets in version control
- [ ] CORS origins restricted to production domain
- [ ] `npm run build` succeeds with production env
- [ ] `uvicorn app.main:app` starts without errors

### URLs

| Service | URL |
|---------|-----|
| Frontend | https://speechi.adirg.dev |
| Backend API | https://speechi.adirg.dev/api |
| Health Check | https://speechi.adirg.dev/api/health |
| Swagger Docs | https://speechi.adirg.dev/api/docs (if enabled) |

### Environment Variables (Production)

**Frontend (`.env.production`):**
```
VITE_API_BASE_URL=https://speechi.adirg.dev/api
VITE_APP_ENV=production
```

**Backend (`.env`):**
```
APP_ENV=production
APP_HOST=0.0.0.0
APP_PORT=8000
CORS_ORIGINS=https://speechi.adirg.dev
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
```
